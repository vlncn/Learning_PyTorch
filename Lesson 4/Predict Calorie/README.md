This is the project that I did after watching the lesson 4 of fast.ai Course. I want to join two kaggle competitions. Predict Calorie and something related with NLP. The lesson actually talked about NLP, its future and how to train an NLP Model. I think at the time of recording, GPT-3 Hasn't been released yet, the professor still talk about GPT-2. GPT-2 is already impressive. But, GPT-3 is miles better.

Back to the main thought process of me doing this kind of project. The first I did is the Predict Calorie. As I'm training model, the deadline for the competition is actually just 19 hours left!!. So I just did some rough training without care and just do the "ideal" way of training a ML model. just input, train, and output and then it's done. I know its pretty naive to do things this way so I look on how others tackle this problem.

It's turns out this competition is great for learning how to do feature engineering. My first thought when I look at the data label is "I need to add Body Mass Index(BMI) as a variable to the model because there's a data on height and weight". Well everyone also thought so, but they did it systematically, first they clean the data of any duplicate, missing or NaN data. Then, they look on how the data graph looks like. And then, they trained the model just to look on how the model perform ("well the error is actually decent but they know that it can get better") So they look on how a variable connect(?) with each other and start from there to create a new feature. BMI is one of them and there's also Body Mass Ratio and Intensity Heart feature. I add them to my code and voila the score improved From 0.061 to 0.060; Well it's a improvement. Anyways, I use the improved model to predict the test set. I also calculate the BMI, BMR, and Intensity Heart to the test set and start predicting using the improved model. 

I submit the prediction to the competition. I don't have a high hope of getting a good score because my previous Titanic competition. And to my suprise the model actually perform better by getting the 0.058 on kaggle competition. So my next goal would be to make my score 0.057. But, I want to experiment first on how about when we already derive the new feature, Maybe, we don't need the original data so I deleted the feature Height, Weight, Heart rate, and duration. Train the model again and to no suprise, the model performs worse. So we basically need the original and the derivative data to make the model better. I also want to know how other algorithm perform so i use another algorithm called Catboost and It performed worse.

The takeaway I got from this project is actually I'm overwhelmed by feature engineering. I used to look down on that because I just thought it was easy, just 3 line of code then you have new feature to feed to your model. Turns out its so much more than that and you need to look on how the variable connect(?) with each other. I actually saw some code of the feature engineering and was so confused on what code do what. well it's a lesson I need to learn another day.

My other takeaway is how badly i'm at documenting stuff. The jupyter lab file is actually just the Catboost experiment and my deleting original feature and using the XGBoost Regressor is not written on file. the submission used the cat boost model and the original using XGBoost and the deleted original feature is just gone.

For the next one is NLP, I actually lost on choosing the US Patent NLP like the lesson teach or just a new competition. 