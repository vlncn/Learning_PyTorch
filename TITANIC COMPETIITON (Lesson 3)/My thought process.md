This is my thought process after watching the lesson 3 of fast ai course. Lesson teachs me on behind the scene of a Deep Learning model. It's actually just a bunch of ReLU (Rectifier Linear Unit) and how a model learn just by doing gradient descent. The professor use an interactive slider to basically change a parameter of a function till it was close to the original function. I also learn the meaning behind the hyperparamater. During my ML class in college, I'm confused by what is a hyperparameter and gradient descent. This course cleared that up for me. 

At the end of class, the professor teaches on how an AI learn by hand(?) at least in a way a simpler one just without a bunch of parameter that usually a model would use. The data is from a Titanic sinking on who survives or not. And my task after this lesson would be to use that data to train a model and join the kaggle competition.

The kaggle competition is basically, here's some data, train a model, use the model to predict the test data, save the prediction data into an .csv file. Downloading the data is done in kaggle website. For the model, I use the XGBoost model for this task.

Well, it's been so long since I trained a ML model so I need some refresher on how to use pandas library and how to clean some data. Well, first  things that I do is always look at the data using pandas df.info() command. Turns out, he data that I got from kaggle there's a bunch of data of passager without an age. Well, most of data we got in real life also looks like that, there's now way you're getting a data where everything is clean. 

So first I just delete the data of passager without age data. Afterwards, I choose my paramater from the label. There's no need to feed identification into the model so I don't use the label [Name, Cabin Number, passagerId, Ticket]. I also don't think that a person with a siblings or come with parent means much so I don't use the label Sibling and Parents.

So, it leaves me with the label [Age, Embarked, Pclass, Sex, Fare] as my parameter to train my model. The data need to be readable by a computer, so I transform the data into numerical values using the pandas library. During this process of code, I forgot most the pandas library so I'm looking into the documentation and searching google for on how to do some things using the pandas library. The transformations that I did mostly follow the lesson 3 on how he transform the data. Then the transformations, for Sex there's only Male and Female as an output so i mapped them into male equal 0 and female equal 1. For, Embarked and Pclass I use the get_dummies command from pandas library to turn the categorical values of the label into numerical by transforming them into a boolean label. As for age and fare, for Age, the professor said that because it uses a big number it can create a bias for the model so the bigger the age the the guess rate become higher(Maybe?). So the age is divided by its max value. and for fare, there's an unequal distributions of fare like there's some really high fare and really low fare, so the professor use some logaritmic distributions and I just follow that example.

After successfully transforming and cleaning the data (Finally!). I step into the next one, which is just train the model. It's done in under a minute. Well, most of data science works like this. I use XGBoost at first and when I trained it and use a train_test_split, I got a model with the accuracy of 87%. For the first try it wasn't so bad. I tried to make it better and then I just use all of the data from the train set into the model. I'm still naive at this point of time so I just the data from the train set as a test without removing it from the train set (Actually, please don't do this). Of course the accuracy will be high, it's 97% accuracy. Feeling pretty confident, I use that model to predict using the test set and submit it to the kaggle competition. 

I'm confident I at least get 80% score or something. Well, I only got 59%. So I was a bit dissapointed but I know i'm doing some really silly stuff so I'm back in the drawing board and change the model to use Random Forest. Train the model again and i got an improvement, the score about 66. Well i know it could be better started looking into the test and found out that some of the data is a garbage and there's some missing value in some of the age and fare label. Well, i actually don't what the correct action to do for this kind of thing. So, to improve my I just tweak the data and fill the garbage and missing age and fare value with its mean. Following that, I predict again and my score improved again. This time its 76%. Well i'm already happy with so that's the end of this task. 